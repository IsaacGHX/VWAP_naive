import os
import pandas as pd


def consolidate_minute_data(root_dir, target_symbol, output_dir, output_file):
    """
    ã€æ–°é€»è¾‘ã€‘éå†åˆ†é’Ÿæ•°æ®ç›®å½•ç»“æ„ (YYYYMM/YYYYMMDD/symbol.csv)ï¼Œ
    ä¸ºç›®æ ‡åˆçº¦ç”Ÿæˆä¸€ä¸ªåˆå¹¶çš„CSVæ–‡ä»¶ã€‚

    :param root_dir: åŒ…å«YYYYMMæ–‡ä»¶å¤¹çš„æ ¹ç›®å½•ã€‚
    :param target_symbol: éœ€è¦æŠ½å–çš„åˆçº¦ä»£ç ï¼Œä¾‹å¦‚ 'IF9999'ã€‚
    :param output_dir: è¾“å‡ºæ•°æ®è¦å­˜æ”¾çš„ç›®å½•ã€‚
    :param output_file: è¾“å‡ºçš„CSVæ–‡ä»¶åã€‚
    """
    print(f"ğŸš€ å¼€å§‹å¤„ç†åˆ†é’Ÿçº§æ•°æ®ï¼Œç›®æ ‡åˆçº¦: {target_symbol}")
    print(f"ğŸ“‚ æ‰«ææ ¹ç›®å½•: {os.path.abspath(root_dir)}")

    all_symbol_data = []
    files_found = 0
    target_filename = f"{target_symbol}.csv"  # æˆ‘ä»¬è¦å¯»æ‰¾çš„ç›®æ ‡æ–‡ä»¶å

    # ä½¿ç”¨os.walkéå†æ‰€æœ‰å­ç›®å½•å’Œæ–‡ä»¶
    for dirpath, _, filenames in os.walk(root_dir):
        # æ£€æŸ¥å½“å‰æ–‡ä»¶å¤¹ä¸­æ˜¯å¦å­˜åœ¨æˆ‘ä»¬çš„ç›®æ ‡æ–‡ä»¶
        if target_filename in filenames:
            file_path = os.path.join(dirpath, target_filename)
            try:
                # è¯»å–è¯¥åˆçº¦çš„åˆ†é’Ÿæ•°æ®æ–‡ä»¶
                minute_df = pd.read_csv(file_path)

                # æ•´ä¸ªæ–‡ä»¶éƒ½æ˜¯æˆ‘ä»¬éœ€è¦çš„æ•°æ®ï¼Œç›´æ¥æ·»åŠ åˆ°åˆ—è¡¨ä¸­
                if not minute_df.empty:
                    all_symbol_data.append(minute_df)
                    files_found += 1

            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ {file_path}: {e}")

    if not all_symbol_data:
        print(f"âŒ æœªåœ¨ç›®å½•ä¸­æ‰¾åˆ°ä»»ä½•åä¸º '{target_filename}' çš„æ–‡ä»¶ã€‚")
        return

    print(f"âœ… æˆåŠŸæ‰¾åˆ°å¹¶å¤„ç†äº† {files_found} ä¸ªå…³äº '{target_symbol}' çš„æ•°æ®æ–‡ä»¶ã€‚")

    # å°†æ‰€æœ‰æ‰¾åˆ°çš„DataFrameåˆå¹¶æˆä¸€ä¸ª
    final_df = pd.concat(all_symbol_data, ignore_index=True)

    # --- æ•°æ®æ ¼å¼åŒ– (è¿™éƒ¨åˆ†é€»è¾‘ä¸æ—¥çº¿æ•°æ®å®Œå…¨ç›¸åŒ) ---

    # 1. å®šä¹‰éœ€è¦ä¿ç•™çš„åŸå§‹åˆ—åå’Œæ–°çš„å°å†™åˆ—å
    columns_map = {
        'eob': 'date',
        'open': 'open',
        'close': 'close',
        'high': 'high',
        'low': 'low',
        'volume': 'volume',
        'amount': 'amount',
        'position': 'position'
    }

    # 2. åªä¿ç•™éœ€è¦çš„åˆ—ï¼Œå¹¶é‡å‘½å
    #    ä¸ºé˜²æ­¢æŸäº›æ–‡ä»¶åˆ—ä¸å…¨ï¼Œå…ˆç­›é€‰å‡ºå®é™…å­˜åœ¨çš„åˆ—
    existing_cols = [col for col in columns_map.keys() if col in final_df.columns]
    final_df = final_df[existing_cols].rename(columns=columns_map)

    # 3. æŒ‰ç…§æ—¥æœŸæ—¶é—´æ’åºï¼Œç¡®ä¿æ•°æ®æ˜¯æ—¶åºæ­£ç¡®çš„
    final_df = final_df.sort_values(by='date', ascending=True)

    # 4. (å¯é€‰) è°ƒæ•´åˆ—çš„é¡ºåºï¼Œæ›´ç¬¦åˆä¹ æƒ¯
    #    è¿™é‡Œçš„åˆ—é¡ºåºä¿æŒå’Œæ—¥çº¿æ•°æ®ä¸€è‡´
    column_order = ['date', 'open', 'high', 'low', 'close', 'volume', 'amount', 'position']
    final_df = final_df[column_order]

    # --- ä¿å­˜æ–‡ä»¶ ---
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, output_file)
    final_df.to_csv(output_path, index=False)
    print(f"ğŸ‰ æˆåŠŸï¼åˆ†é’Ÿçº§æ•°æ®å·²ä¿å­˜è‡³: {output_path}")
    print(f"æ€»è®¡ {len(final_df)} è¡Œæ•°æ®ã€‚")


# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    # --- æ‚¨éœ€è¦ä¿®æ”¹çš„é…ç½® ---

    # 1. åˆ†é’Ÿçº§æ•°æ®æ‰€åœ¨çš„æ ¹ç›®å½•
    ROOT_DATA_DIR = r'D:\Desktop\SHU\Intern\åŒæ¢AIé‡åŒ–\database\1min\output'  # <-- è¯·ä¿®æ”¹ä¸ºæ‚¨çš„åˆ†é’Ÿæ•°æ®æ ¹ç›®å½•

    # 2. æ‚¨æƒ³è¦æŠ½å–çš„åˆçº¦ä»£ç 
    TARGET_SYMBOL = 'c9999'  # <-- ä¿®æ”¹ä¸ºæ‚¨éœ€è¦çš„åˆçº¦ï¼Œä¾‹å¦‚ 'c9999'

    # 3. å®šä¹‰è¾“å‡ºç›®å½•
    OUTPUT_DIR = "./data/raw_data"  # <-- è¾“å‡ºæ•°æ®å°†ä¿å­˜åœ¨æ­¤æ–‡ä»¶å¤¹

    # 4. è¾“å‡ºæ–‡ä»¶çš„åç§°
    OUTPUT_FILENAME = f'{TARGET_SYMBOL}_1min_data.csv'

    # --- æ‰§è¡Œå‡½æ•° ---
    consolidate_minute_data(
        root_dir=ROOT_DATA_DIR,
        target_symbol=TARGET_SYMBOL,
        output_dir=OUTPUT_DIR,
        output_file=OUTPUT_FILENAME
    )